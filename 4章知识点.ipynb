{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "知识点注释以及解析\n",
    "4-1：\n",
    "二次代价函数（quadratic cost）：lab值减去预测值的平方累加除以2n\n",
    "预测值的求法：上一层神经元的 W * X + b 的累加和再经过激活函数\n",
    "该函数的调整与激活函数的导数成正比\n",
    "\n",
    "交叉熵代价函数（cross-entropy）：C = -1/n E(代表求和函数)（ylna + (1 - y)ln(1 - a)）\n",
    "该函数的调整与输出值与预测值的误差大小有关\n",
    "\n",
    "如果输出神经元是线性的，则选用二次代价函数；如果输出神经元为s型（即激活函数为s型），则选用交叉熵代价函数\n",
    "\n",
    "对数释然代价函数（log-likelihood cost）作为softmax回归的代价函数，在二分类时可以化简为交叉熵代价函数\n",
    "\n",
    "Tensorflow中：\n",
    "tf.nn.sigmoid_cross_entropy_with_logits()表示跟sigmoid搭配使用的交叉熵\n",
    "tf.nn.softmax_cross_entropy_with_logits()表示跟softmax搭配使用的交叉熵\n",
    "\n",
    "\n",
    "4-2：\n",
    "解决过拟合的方法：\n",
    "1.增加数据集\n",
    "2.正则化方法，即加上一个项 c = c0 + [(lamuda)/2*n]*累加w*w  \n",
    "lamuda可以调整加入项的占比  c指的是损失函数  这样做的目的是同时优化权值w，使w越来越小\n",
    "当某些权值变为0的时候，进而减少网络的复杂程度，防止过拟合\n",
    "3.dropout ：以一定的概率挑选神经元，训练的时候使用部分神经元进行训练，验证的时候，\n",
    "使用全部神经元进行操作。其本质作用是使得所有神经元都有可能不工作，这样使每个神经元所占权重不会过大\n",
    "\n",
    "\n",
    "4-3：\n",
    "优化器（Optimizer）：\n",
    "常见：\n",
    "tf.train.GradientDescentOptimizer #梯度下降法\n",
    "tf.train.AdadeltaOptimizer        #\n",
    "#Adadelta 甚至不需要设置默认的学习率，不使用学习率也可以达到较好效果\n",
    "tf.train.AdagradDAOptimizer\n",
    "tf.train.AdagradOptimizer\n",
    "#Adagrad 基于SGD 核心思想：对比较常见的数据给予比较小的学习率去调整参数\n",
    "#对比较罕见的数据给予比较大的学习率去调整参数，适合应用于数据稀疏的数据集（不同类别数据基数不同）\n",
    "#优点：自动调整学习率，数据较大时，后期学习率为0\n",
    "tf.train.AdamOptimizer            #\n",
    "#Adam 保持衰减梯度，更新参数    使用这个学习率一般都比较小，如0.001\n",
    "tf.train.MomentumOptimizer        #\n",
    "#momentum当前权值的改变会受到上一次权值的影响类似小球向下滚动要带惯性，使收敛更快\n",
    "#NAG 更聪明的小球，在底部会减缓速度\n",
    "tf.train.FtrlOptimizer\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "tf.train.RMSPropOptimizer         #\n",
    "#RMSprop 借鉴Adagrad思想 消除了学习率越来越低的问题，可以拥有一个较好结果\n",
    "\n",
    "#综合比较来看：Adadelta优化器收敛速度最快  NAG也是较快的算法（可能会走弯路）\n",
    "#准确率则不一定，具体看具体分析进行操作\n",
    "标准梯度下降法：先计算所有样本汇总误差，然后根据总误差来更新权值\n",
    "\n",
    "随机梯度下降法：随机抽取一个样本来计算误差，然后更新权值\n",
    "\n",
    "批量梯度下降法：一种折中的方案，从总样本中选取一个批次，然后计算这个批次的总误差\n",
    "根据总误差来更新权值\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
